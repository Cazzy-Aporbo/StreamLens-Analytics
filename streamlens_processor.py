#!/usr/bin/env python3
"""
StreamLens Analytics Data Processing Pipeline
Advanced representation and bias analysis for streaming media content
Author: Cazandra Aporbo, MS
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime
import json
import re
from collections import defaultdict, Counter
import networkx as nx
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, IsolationForest
from sklearn.tree import DecisionTreeClassifier, export_text
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import classification_report
import warnings
warnings.filterwarnings('ignore')

# Note: Removed spaCy and transformers imports to avoid dependency conflicts
# This simplified version focuses on the core statistical analysis

@dataclass
class MediaContent:
    """Represents a media content item with metadata"""
    id: str
    title: str
    platform: str
    genre: str
    release_year: int
    episodes: int
    runtime: float
    cast: List[Dict]
    dialogue_data: Optional[Dict] = None
    
class RepresentationAnalyzer:
    """Main analyzer for representation metrics"""
    
    def __init__(self):
        self.scaler = RobustScaler()  # Robust to outliers
        self.bias_detector = BiasDetector()
        self.network_analyzer = NetworkAnalyzer()
        
    def calculate_diversity_index(self, demographics: List[str]) -> float:
        """Calculate Shannon diversity index"""
        counts = Counter(demographics)
        total = sum(counts.values())
        if total == 0:
            return 0
        
        diversity = 0
        for count in counts.values():
            p = count / total
            if p > 0:
                diversity -= p * np.log(p)
        
        return diversity / np.log(len(counts)) if len(counts) > 1 else 0
    
    def calculate_gender_parity(self, gender_data: Dict) -> float:
        """Calculate gender parity index (0-1, where 1 is perfect parity)"""
        total = sum(gender_data.values())
        if total == 0:
            return 0
        
        female_ratio = gender_data.get('female', 0) / total
        return 1 - abs(0.5 - female_ratio) * 2
    
    def calculate_intersectionality_score(self, data: pd.DataFrame) -> Dict:
        """Calculate intersectional representation scores"""
        intersections = {}
        
        # Define intersectional groups
        for gender in ['male', 'female', 'non-binary']:
            for race in ['white', 'black', 'asian', 'hispanic', 'other']:
                for age in ['<30', '30-50', '>50']:
                    group_key = f"{gender}_{race}_{age}"
                    
                    # Calculate representation vs population baseline
                    group_data = data[
                        (data['gender'] == gender) & 
                        (data['race'] == race) & 
                        (data['age_group'] == age)
                    ]
                    
                    if len(group_data) > 0:
                        representation = len(group_data) / len(data)
                        # Simplified population baseline (would use real census data)
                        population_baseline = 1 / (3 * 5 * 3)  
                        intersections[group_key] = {
                            'representation': representation,
                            'baseline': population_baseline,
                            'ratio': representation / population_baseline if population_baseline > 0 else 0,
                            'screen_time': group_data['screen_time'].mean() if 'screen_time' in group_data else 0
                        }
        
        return intersections

class BiasDetector:
    """Advanced bias detection using statistical and ML methods"""
    
    def __init__(self):
        self.models = {
            'gender_bias': RandomForestClassifier(n_estimators=100, random_state=42),
            'racial_bias': GradientBoostingClassifier(n_estimators=100, random_state=42),
            'age_bias': RandomForestClassifier(n_estimators=100, random_state=42)
        }
        
    def detect_dialogue_bias(self, dialogue_data: Dict) -> Dict:
        """Detect bias in dialogue distribution and sentiment"""
        biases = {}
        
        # Analyze speaking time distribution
        total_words = sum(dialogue_data.get('word_counts', {}).values())
        
        for demographic, word_count in dialogue_data.get('word_counts', {}).items():
            expected = total_words / len(dialogue_data['word_counts'])
            observed = word_count
            
            # Chi-square test statistic
            chi_square = ((observed - expected) ** 2) / expected if expected > 0 else 0
            
            biases[f'dialogue_{demographic}'] = {
                'observed': observed,
                'expected': expected,
                'bias_score': chi_square,
                'percentage': (observed / total_words * 100) if total_words > 0 else 0
            }
        
        return biases
    
    def detect_role_stereotyping(self, character_data: pd.DataFrame) -> Dict:
        """Detect stereotypical role assignments"""
        stereotypes = {}
        
        role_demographics = character_data.groupby(['role_type', 'gender']).size().unstack(fill_value=0)
        
        for role in role_demographics.index:
            role_data = role_demographics.loc[role]
            total = role_data.sum()
            
            if total > 0:
                # Calculate deviation from expected distribution
                expected = total / len(role_data)
                chi_square = sum(((role_data - expected) ** 2) / expected)
                
                stereotypes[role] = {
                    'distribution': role_data.to_dict(),
                    'chi_square': chi_square,
                    'stereotype_score': min(chi_square / 10, 1),  # Normalized score
                    'dominant_gender': role_data.idxmax()
                }
        
        return stereotypes
    
    def calculate_bechdel_extension(self, interactions: List[Dict]) -> Dict:
        """Extended Bechdel test for multiple dimensions"""
        results = {
            'classic_bechdel': False,
            'racial_bechdel': False,
            'age_bechdel': False,
            'lgbtq_bechdel': False
        }
        
        # Classic Bechdel: Two women talking about something other than a man
        female_interactions = [i for i in interactions if 
                              i.get('char1_gender') == 'female' and 
                              i.get('char2_gender') == 'female']
        
        non_romantic = [i for i in female_interactions if 
                       'romantic' not in i.get('topic', '').lower() and
                       'relationship' not in i.get('topic', '').lower()]
        
        results['classic_bechdel'] = len(non_romantic) > 0
        
        # Similar tests for other dimensions
        minority_interactions = [i for i in interactions if 
                                i.get('char1_race') != 'white' and 
                                i.get('char2_race') != 'white']
        
        results['racial_bechdel'] = len(minority_interactions) > 5  # Threshold
        
        return results

class NetworkAnalyzer:
    """Analyze character interaction networks"""
    
    def build_character_network(self, interactions: List[Dict]) -> nx.Graph:
        """Build network graph from character interactions"""
        G = nx.Graph()
        
        for interaction in interactions:
            char1 = interaction['character1']
            char2 = interaction['character2']
            
            # Add nodes with attributes
            if not G.has_node(char1):
                G.add_node(char1, **interaction.get('char1_attrs', {}))
            if not G.has_node(char2):
                G.add_node(char2, **interaction.get('char2_attrs', {}))
            
            # Add or update edge
            if G.has_edge(char1, char2):
                G[char1][char2]['weight'] += interaction.get('weight', 1)
            else:
                G.add_edge(char1, char2, weight=interaction.get('weight', 1))
        
        return G
    
    def calculate_centrality_metrics(self, G: nx.Graph) -> pd.DataFrame:
        """Calculate various centrality metrics for characters"""
        metrics = pd.DataFrame()
        
        metrics['degree'] = pd.Series(dict(G.degree()))
        metrics['betweenness'] = pd.Series(nx.betweenness_centrality(G))
        metrics['eigenvector'] = pd.Series(nx.eigenvector_centrality(G, max_iter=1000))
        metrics['closeness'] = pd.Series(nx.closeness_centrality(G))
        
        # Add demographic data
        node_attrs = nx.get_node_attributes(G, 'gender')
        metrics['gender'] = pd.Series(node_attrs)
        
        node_attrs = nx.get_node_attributes(G, 'race')
        metrics['race'] = pd.Series(node_attrs)
        
        return metrics
    
    def detect_homophily(self, G: nx.Graph, attribute: str) -> float:
        """Calculate homophily (tendency for similar nodes to connect)"""
        try:
            return nx.attribute_assortativity_coefficient(G, attribute)
        except:
            return 0.0

class DataProcessor:
    """Main data processing pipeline"""
    
    def __init__(self):
        self.analyzer = RepresentationAnalyzer()
        self.platforms = ['netflix', 'amazon_prime', 'disney_plus', 'hbo_max', 'apple_tv', 'hulu']
        self.genres = ['drama', 'comedy', 'action', 'scifi', 'thriller', 'documentary']
        
    def generate_synthetic_data(self, n_samples: int = 1000) -> pd.DataFrame:
        """Generate synthetic data for demonstration"""
        np.random.seed(42)
        
        data = []
        
        for i in range(n_samples):
            year = np.random.randint(2015, 2025)
            platform = np.random.choice(self.platforms)
            genre = np.random.choice(self.genres)
            
            # Create realistic bias patterns
            gender_bias = 0.3 if genre == 'action' else 0.1
            age_bias = 0.2 if genre == 'drama' else 0.05
            
            # Generate character
            gender = np.random.choice(['male', 'female', 'non-binary'], 
                                    p=[0.5 + gender_bias, 0.5 - gender_bias - 0.02, 0.02])
            
            race = np.random.choice(['white', 'black', 'asian', 'hispanic', 'other'],
                                  p=[0.6, 0.13, 0.06, 0.18, 0.03])
            
            age_group = np.random.choice(['<30', '30-50', '>50'],
                                       p=[0.4 + age_bias, 0.4, 0.2 - age_bias])
            
            # Calculate metrics with bias
            base_screen_time = 50 + np.random.normal(0, 20)
            screen_time = base_screen_time * (1.2 if gender == 'male' else 0.9 if gender == 'female' else 0.7)
            
            dialogue_words = int(screen_time * 20 + np.random.normal(0, 100))
            
            role_type = np.random.choice(['lead', 'support', 'minor'])
            
            # Sentiment based on role and demographics
            sentiment = 0.7 if role_type == 'lead' else 0.5
            sentiment += np.random.normal(0, 0.1)
            sentiment = np.clip(sentiment, 0, 1)
            
            data.append({
                'id': f'char_{i}',
                'platform': platform,
                'genre': genre,
                'year': year,
                'gender': gender,
                'race': race,
                'age_group': age_group,
                'role_type': role_type,
                'screen_time': max(0, screen_time),
                'dialogue_words': max(0, dialogue_words),
                'sentiment_score': sentiment,
                'centrality': np.random.beta(2, 5),  # Skewed distribution
                'is_protagonist': role_type == 'lead' and np.random.random() > 0.5
            })
        
        return pd.DataFrame(data)
    
    def process_data(self, df: pd.DataFrame) -> Dict:
        """Process data and calculate all metrics"""
        results = {
            'overall_metrics': {},
            'temporal_analysis': {},
            'platform_comparison': {},
            'genre_analysis': {},
            'bias_detection': {},
            'network_metrics': {}
        }
        
        # Overall metrics
        results['overall_metrics']['diversity_index'] = self.analyzer.calculate_diversity_index(
            df['race'].tolist()
        )
        
        gender_counts = df['gender'].value_counts().to_dict()
        results['overall_metrics']['gender_parity'] = self.analyzer.calculate_gender_parity(
            gender_counts
        )
        
        results['overall_metrics']['intersectionality'] = self.analyzer.calculate_intersectionality_score(df)
        
        # Temporal analysis
        yearly_metrics = []
        for year in df['year'].unique():
            year_data = df[df['year'] == year]
            yearly_metrics.append({
                'year': year,
                'diversity': self.analyzer.calculate_diversity_index(year_data['race'].tolist()),
                'gender_parity': self.analyzer.calculate_gender_parity(
                    year_data['gender'].value_counts().to_dict()
                ),
                'avg_sentiment': year_data['sentiment_score'].mean()
            })
        
        results['temporal_analysis'] = pd.DataFrame(yearly_metrics).to_dict('records')
        
        # Platform comparison
        platform_metrics = []
        for platform in df['platform'].unique():
            platform_data = df[df['platform'] == platform]
            platform_metrics.append({
                'platform': platform,
                'diversity': self.analyzer.calculate_diversity_index(platform_data['race'].tolist()),
                'female_leads': (platform_data[
                    (platform_data['gender'] == 'female') & 
                    (platform_data['role_type'] == 'lead')
                ].shape[0] / platform_data[platform_data['role_type'] == 'lead'].shape[0]) 
                if platform_data[platform_data['role_type'] == 'lead'].shape[0] > 0 else 0,
                'avg_screen_time_disparity': platform_data.groupby('gender')['screen_time'].mean().std()
            })
        
        results['platform_comparison'] = pd.DataFrame(platform_metrics).to_dict('records')
        
        # Bias detection
        bias_detector = BiasDetector()
        results['bias_detection']['role_stereotyping'] = bias_detector.detect_role_stereotyping(df)
        
        # Dialogue bias simulation
        dialogue_data = {
            'word_counts': df.groupby('gender')['dialogue_words'].sum().to_dict()
        }
        results['bias_detection']['dialogue_bias'] = bias_detector.detect_dialogue_bias(dialogue_data)
        
        return results
    
    def export_results(self, results: Dict, filename: str = 'analysis_results.json'):
        """Export results to JSON file"""
        # Convert numpy types for JSON serialization
        def convert_types(obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {k: convert_types(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_types(i) for i in obj]
            return obj
        
        clean_results = convert_types(results)
        
        with open(filename, 'w') as f:
            json.dump(clean_results, f, indent=2)
        
        print(f"Results exported to {filename}")
        
        return clean_results

def main():
    """Main execution function"""
    print("=" * 60)
    print("StreamLens Analytics - Streaming Media Bias Analysis")
    print("Author: Cazandra Aporbo, MS")
    print("=" * 60)
    
    # Initialize processor
    processor = DataProcessor()
    
    # Generate synthetic data
    print("\nGenerating synthetic dataset...")
    df = processor.generate_synthetic_data(n_samples=5000)
    print(f"Generated {len(df)} character records")
    
    # Process data
    print("\nAnalyzing representation and bias...")
    results = processor.process_data(df)
    
    # Display key findings
    print("\nKey Findings:")
    print("-" * 40)
    
    overall = results['overall_metrics']
    print(f"Diversity Index: {overall['diversity_index']:.3f}")
    print(f"Gender Parity: {overall['gender_parity']:.3f}")
    
    # Temporal trends
    temporal = pd.DataFrame(results['temporal_analysis'])
    if not temporal.empty:
        trend = temporal['diversity'].iloc[-1] - temporal['diversity'].iloc[0]
        print(f"Diversity Trend (10yr): {'+' if trend > 0 else ''}{trend:.3f}")
    
    # Platform comparison
    platforms = pd.DataFrame(results['platform_comparison'])
    if not platforms.empty:
        best_platform = platforms.loc[platforms['diversity'].idxmax(), 'platform']
        best_score = platforms['diversity'].max()
        print(f"Most Diverse Platform: {best_platform.replace('_', ' ').title()} ({best_score:.3f})")
    
    # Bias detection
    print("\nBias Detection Results:")
    print("-" * 40)
    
    dialogue_bias = results['bias_detection']['dialogue_bias']
    for demo, bias_data in dialogue_bias.items():
        if 'percentage' in bias_data:
            print(f"{demo}: {bias_data['percentage']:.1f}% of dialogue")
    
    # Export results
    print("\nExporting results...")
    processor.export_results(results)
    
    print("\nAnalysis complete!")
    print("View the interactive dashboard by opening index.html in your browser")

if __name__ == "__main__":
    main()